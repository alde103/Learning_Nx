# Nx Tips

```elixir
Mix.install(
  [{:nx, "~> 0.5"}] 
)
```

## Nx Tip of the Week #1 – Using transforms

Nx is a tensor manipulation or array programming library similar to NumPy, TensorFlow, or PyTorch.

Nx introduces a new type of function definition, defn, that is a subset of the Elixir programming language tailored specifically to numerical computations. When numerical definitions are invoked, they’re transformed into expressions (internally Nx.Defn.Expr) which represent the AST or computation graph of the numerical definition. These expressions are manipulated by compilers (like EXLA) to produce executables that run natively on accelerators.

In newer Nx versions, transform/2 has been removed, so its a better practice to let all math be handled by Nx and the business logic in the Elixir realm.

Use print_expr inside a defn expression to debug the operation.

```elixir
defmodule NxTest do
  import Nx.Defn

  def cross_entropy_loss(y_true, y_pred) do
    check_shape(Nx.shape(y_true), Nx.shape(y_pred))
    nx_cross_entropy_loss(y_true, y_pred)
  end
  
  defp check_shape(same_shape, same_shape), do: nil
  defp check_shape(_y_true_shape, _y_pred_shape), 
    do: raise(ArgumentError, "shapes do not equal")
  
  defn tanh_power(a, b) do
    exp = Nx.tanh(a) + Nx.pow(b, 2)
    print_expr(exp)
  end

  defn nx_cross_entropy_loss(y_true, y_pred) do
    Nx.mean(Nx.log(y_true) * y_pred)
  end
end
```

```elixir
a = Nx.tensor(0)
b = Nx.tensor(2)
NxTest.tanh_power(a, b)
```

```elixir
# To validate that is a Tensor.
{is_struct(a, Nx.Tensor), is_struct(a, Nx)}
```

```elixir
y_true = Nx.tensor([10, 10])
y_pred = Nx.tensor([100, 100])
NxTest.cross_entropy_loss(y_true, y_pred)
```

```elixir
y_true = Nx.tensor([10, 10])
y_pred = Nx.tensor([100])
NxTest.cross_entropy_loss(y_true, y_pred)
```

## Nx Tip of the Week #2 – Tensor Operations for Elixir Programmers

In Elixir, it’s common to manipulate data using the Enum module.

**Element-wise unary functions**

Nx contains a number of element-wise unary functions that are tensor aware.

```elixir
# Enum way
a = [1, 2, 3]
Enum.map(a, fn x -> :math.exp(x) end)
```

```elixir
# Nx way (some operation must specify the output type).
a = Nx.tensor([1, 2, 3], type: {:s, 32}, names: [:data])
Nx.map(a, [type: {:f, 32}], fn x -> Nx.exp(x) end)
# Nx.map is not compatible for most Nx compilers.
```

```elixir
# Must of Nx functions are tensor aware, 
# therefore efficient for most Nx compilers
a = Nx.tensor([1, 2, 3], type: {:f, 32}, names: [:data])
Nx.exp(a)
```

```elixir
a = Nx.iota({2, 2, 1, 2, 1, 2}, type: {:f, 32})
Nx.exp(a)
```

**Element-wise binary functions**

```elixir
# With Enum
a = [1, 2, 3]
b = [4, 5, 6]
a |> Enum.zip(b) |> Enum.map(fn {x, y} -> x + y end)
```

```elixir
# In Nx (there is no zip)

a = Nx.tensor([1, 2, 3], type: {:f, 32})
b = Nx.tensor([4, 5, 6], type: {:f, 32})

Nx.add(a, b)
```

```elixir
a = Nx.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], type: {:f, 32})
b = Nx.tensor([[[2, 3, 4], [5, 6, 7], [8, 9, 10]]], type: {:f, 32})
Nx.add(a, b)
```

```elixir
broadcast_scalar = fn x, list -> Enum.map(list, & &1*x) end
broadcast_scalar.(5, [1, 2, 3])
```

```elixir
broadcast_scalar = &Nx.multiply(&1, &2)
broadcast_scalar.(5, Nx.tensor([1, 2, 3], type: {:f, 32}))
```

```elixir
Nx.multiply(5, Nx.tensor([1, 2, 3], type: {:f, 32}))
```

**Aggregate Operators**

```elixir
# With Enum
a = [1, 2, 3]
Enum.reduce(a, 0, fn x, acc -> x + acc end)
```

```elixir
# Multi-dimensional
a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

Enum.reduce(a, 0, fn x, acc ->
  Enum.reduce(x, 0, fn y, inner_acc ->
    y + inner_acc
  end) + acc
end)
```

```elixir
a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

Enum.reduce(a, [], fn x, acc ->
  [
    Enum.reduce(x, 0, fn y, inner_acc ->
      y + inner_acc
    end)
    | acc
  ]
end)
|> Enum.reverse()
```

```elixir
# With Nx, there is Nx.reduce, however, similar to Nx.map, avoid using it, 
# use native Nx implementations.

a = Nx.tensor([1, 2, 3], type: {:f, 32})

Nx.reduce(a, 0, fn x, acc -> Nx.add(x, acc) end)
```

```elixir
# Multiple dimensions
a = Nx.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], type: {:f, 32})
Nx.sum(a)
```

```elixir
# Through a specific axis
a = Nx.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], type: {:f, 32})
Nx.sum(a, axes: [1])
```

```elixir
Nx.sum(a, axes: [0])
```

```elixir
# Multiple Axis.
Nx.sum(a, axes: [0, 1])
```

```elixir
# Named axis
a = Nx.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], names: [:x, :y, :z], type: {:f, 32})
```

```elixir
Nx.sum(a, axes: [:x])
```

```elixir
Nx.sum(a, axes: [:y])
```

```elixir
Nx.sum(a, axes: [:z])
```

```elixir
Nx.sum(a, axes: [:z, :y]) == Nx.sum(a, axes: [:y, :z])
```

Notice how in each case the axis that disappears is the one provided in axes. axes also supports negative indexing; however, you should generally prefer using named axes over integer axes wherever possible.

## Nx Tip of the Week #3 – Many Ways to Create Arrays* (*Tensors)

In Nx, the fundamental type is the Tensor. You can think of a tensor as a multi-dimensional array, like the numpy.ndarray.

```elixir
# this is the most tempted to use, but you should usually try to avoid.
# 0D Tensor
Nx.tensor(1)
```

```elixir
#1D Tensor
Nx.tensor([1.0, 2.0, 3.0])
```

```elixir
#ND Tensor
Nx.tensor([[[[[[[[[[1,2]]]]]]]]]])
```

```elixir
# By default, Nx.Tensor creates tensor with type s64
# when the inputs are all integer types and f32 when the inputs are all float types.
Nx.tensor([1.0, 2])

```

```elixir
# You can also specify the input type and dimension names.
Nx.tensor([1, 2, 3], type: {:bf, 16}, names: [:data])

# Also you can define its backend, but you must added to the dependencies.
#Nx.tensor([1, 2, 3], backend: Torchx.Backend)
```

Using Nx.tensor/2 is convenient, but is generally less efficient than other methods. Nx.Tensor generally represents tensor data as binaries, so Nx.tensor/2 needs to iterate through the entire list and rewrite it to a binary. You should avoid this, if possible.

**From Binaries**

Instead of creating tensors from lists, you should try to create tensors from binaries.

Tensor data is generally stored in a binary. Using native manipulation is usually more efficient than with other data types.

```elixir
Nx.from_binary(<<0::64-signed-native>>, {:s, 64})
```

```elixir
Nx.from_binary(<<0::32-float-native>>, {:f, 32})

# Notice Nx.from_binary/2 requires the input type and infers the shape as a flat list. 

# Note: You’ll likely want to brush up on binary pattern matching, 
# creation, and manipulation as you work with Nx.
```

```elixir
Nx.from_binary(<<1::64-float-native>>, {:f, 64})
```

```elixir
Nx.from_binary(<<1::64-float-native>>, {:f, 32})
```

```elixir
Nx.from_binary(<<1::64-float-native>>, {:s, 64})
```

```elixir
Nx.from_binary(<<1::64-float-native>>, {:u, 8})
```

```elixir
t = Nx.from_binary(<<1::64-float-native>>, {:f, 64})
Nx.reshape(t, {1, 1, 1, 1})

# Nx.reshape/2 is actually just a meta operation. 
# The implementation doesn’t move the underlying bytes at all, 
# it just changes the shape property of the input tensor.

# You just need to know the shape of the input data.
```

**Broadcasting**

```elixir
# np.full, np.full_like -> Nx.broadcast
 zeros = Nx.broadcast(0, {2, 5})
```

```elixir
ones_like_zeros = Nx.broadcast(1, zeros)
```

```elixir
# If you want to dictate the output type

Nx.broadcast(Nx.tensor(0, type: {:bf, 16}), {2, 2})
```

**Counting Up**

```elixir
# Nx.iota/2. Nx.iota/2 is like np.arange

Nx.iota({2, 5}, axis: 1)
```

```elixir
Nx.iota({2, 5})
```

```elixir
Nx.iota({1}, type: {:bf, 16}, names: [:data])
```

```elixir
Nx.multiply(Nx.iota({2, 5}, axis: 1), 3)
```

```elixir
# Eye matrix
Nx.equal(Nx.iota({3, 3}, axis: 0), Nx.iota({3, 3}, axis: 1))
```

**Random Numbers**

```elixir
key = Nx.Random.key(103)

{uniform, _new_key} = Nx.Random.uniform(key, shape: {2, 2}, type: {:f, 32})
```

```elixir
{normal, _new_key} = Nx.Random.normal(key, 0, 5, shape: {2,2})
```

```elixir
# create a random mask

probability = 0.5
{uniform, _new_key} = Nx.Random.uniform(key, shape: {5, 5})
Nx.select(Nx.less_equal(uniform, probability), 0, 1)
```

**Template**

Nx also has a template creation method that defines a template for an expected future value. This is useful for things like ahead-of-time compilation.

```elixir
# You can't perform any operation on this tensor. 
# It exists exclusively to define APIs that say a tensor 
# with a certain type, shape, and names is expected in the future.
t = Nx.template({4, 4, 4}, {:f, 32}, names: [:x, :y, :z])
```
